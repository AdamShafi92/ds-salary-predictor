{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import glob\n",
    "\n",
    "from time import time, sleep\n",
    "from datetime import datetime\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the relevant information is stored. The html id's can be found on the page using Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "focus": false,
    "id": "e915023e-6b0d-4982-af2a-b1e0355f4927"
   },
   "outputs": [],
   "source": [
    "URL = \"https://www.indeed.co.uk/jobs?q=Data+Scientist&l=United+Kingdom&sort=date&start=0\"\n",
    "r = requests.get(URL).text\n",
    "soup = BeautifulSoup(r, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div id=\"searchCountPages\">\n",
       "                     Page 1 of 1,264 jobs</div>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find pages to use later\n",
    "soup.find_all('div', attrs={'id':'searchCountPages'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nDa'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is each card\n",
    "soup.find_all('div', attrs={'class':'jobsearch-SerpJobCard unifiedRow row result'})[0].text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nData Scientist\\nnew'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is each job title\n",
    "soup.find_all('h2', attrs={'class':'title'})[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCKM Analytix'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the company\n",
    "soup.find_all('span', attrs={'class':'company'})[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'London'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the location\n",
    "soup.find_all('span', attrs={'class':'location accessible-contrast-color-location'})[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is also the location - some records do not have this\n",
    "soup.find_all('div', attrs={'class':'location accessible-contrast-color-location'})[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n£35,000 - £37,000 a year'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the salary\n",
    "soup.find_all('span', attrs={'class':'salaryText'})[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/rc/clk?jk=68ad3a99f6005928&fccid=dd616958bd9ddc12&vjs=3'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the full job url\n",
    "soup.find_all('a', attrs={'target':'_blank'},href=True)[0]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPassion for quantitative problem solving and developing data driven solutions to difficult business questions.\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are the first bullet points\n",
    "soup.find_all('ul', attrs={'style':'list-style-type:circle;margin-top: 0px;margin-bottom: 0px;padding-left:20px;'})[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function loops through all of the pages on Indeed. The total number of pages is also scraped to ensure duplicates aren't created. Wait times are included to prevent the website from blocking incoming requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_job_description(url,jobs):\n",
    "    \n",
    "    title = []\n",
    "    company = []\n",
    "    location = []\n",
    "    location2 = []\n",
    "    salary = []\n",
    "    summary = []\n",
    "    urls = []\n",
    "\n",
    "    for job in tqdm(jobs):\n",
    "        url2 = url.format(job,0)\n",
    "        r = requests.get(url2)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        total = int(re.findall(r'\\d*[^A-z\\s]',soup.find('div', attrs={'id':'searchCountPages'}).text.replace(',',''))[1])\n",
    "\n",
    "        for i in range(0,total,50):\n",
    "\n",
    "            start = time()\n",
    "            while time()-start<randrange(4,7): continue\n",
    "\n",
    "            url3=url.format(job,i)\n",
    "            r = requests.get(url3)\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "            for i in soup.find_all('div', attrs={'class':'jobsearch-SerpJobCard unifiedRow row result'}):\n",
    "\n",
    "                try:\n",
    "                    title.append(i.find('h2', attrs={'class':'title'}).text)\n",
    "                except:\n",
    "                    title.append(np.nan)\n",
    "\n",
    "                try:\n",
    "                    company.append(i.find('span', attrs={'class':'company'}).text)\n",
    "                except:\n",
    "                    company.append(np.nan)\n",
    "\n",
    "                try:\n",
    "                    location.append(i.find('span', attrs={'class':'location accessible-contrast-color-location'}).text)\n",
    "                except:\n",
    "                    location.append(np.nan)\n",
    "\n",
    "                try:\n",
    "                    location2.append(i.find('div', attrs={'class':'location accessible-contrast-color-location'}).text)\n",
    "                except:\n",
    "                    location2.append(np.nan)\n",
    "\n",
    "                try:\n",
    "                    salary.append(i.find('span', attrs={'class':'salaryText'}).text)\n",
    "                except:\n",
    "                    salary.append(np.nan)\n",
    "\n",
    "                try:\n",
    "                    summary.append(i.find('ul', attrs={'style':'list-style-type:circle;margin-top: 0px;margin-bottom: 0px;padding-left:20px;'}).text)\n",
    "                except:\n",
    "                    summary.append(np.nan)\n",
    "\n",
    "                try:\n",
    "                    urls.append(i.find('a', attrs={'target':'_blank'},href=True)['href'])\n",
    "                except:\n",
    "                    urls.append(np.nan)\n",
    "\n",
    "    data = pd.DataFrame(zip(title,company,location,location2,salary,summary,urls))\n",
    "    data.columns = ['Title','Company','Location1','Location2','Salary','Summary','url']\n",
    "    data.drop_duplicates(inplace=True)\n",
    "    data.dropna(subset=['Salary'],inplace=True)\n",
    "    print(len(data))\n",
    "    uuid = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "    data.to_csv(f'./scraped_data/base/scraped_jobs_{uuid}.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Company Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar methodology as above is employed but the wait times are longer here as we have to visit each individual company page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_company_data(company_list):\n",
    "    a=[]\n",
    "    b=[]\n",
    "    for j in range(0,len(companies),200):\n",
    "        start1 = time()\n",
    "        while time()-start1<300: continue\n",
    "        for i in tqdm(companies[j:j+200]):\n",
    "            start = time()\n",
    "            while time()-start<randrange(5,10): continue\n",
    "            url = 'https://www.indeed.co.uk/cmp/{}'.format(i)\n",
    "            r = requests.get(url).text\n",
    "            soup = BeautifulSoup(r, 'html.parser')\n",
    "            try:\n",
    "                a.append([i.text for i in soup.find_all('div', attrs={'class':'cmp-AboutMetadata-itemTitle'})])\n",
    "            except:\n",
    "                a.append(np.nan)\n",
    "            try:\n",
    "                b.append([i.text for i in soup.find_all('div', attrs={'class':'cmp-AboutMetadata-itemCotent'})])\n",
    "            except:\n",
    "                b.append(np.nan)\n",
    "    uuid = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "    company_info = pd.DataFrame(zip(companies,a,b))\n",
    "    company_info.to_csv(f'./scraped_data/companies/company_data_{uuid}.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping full descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full job descriptions are scrape here, this is also slow due to visiting every page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_full_desc(url_list):\n",
    "    a=[]\n",
    "    for j in range(0,len(url_list),200):\n",
    "        start1 = time()\n",
    "        while time()-start1<360: continue\n",
    "        for i in tqdm(url_list[j:j+200]):\n",
    "            start = time()\n",
    "            while time()-start<randrange(7,11): continue\n",
    "            url = 'https://www.indeed.co.uk{}'.format(i)\n",
    "            r = requests.get(url).text\n",
    "            soup = BeautifulSoup(r, 'html.parser')\n",
    "            try:\n",
    "                a.append([i.text for i in soup.find_all('div', attrs={'id':'jobDescriptionText'})])\n",
    "            except:\n",
    "                a.append(np.nan)\n",
    "    full_desc = pd.DataFrame(zip(url_list,a))\n",
    "    uuid = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "    full_desc.to_csv(f'./scraped_data/descriptions/full_desc_{uuid}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.indeed.co.uk/jobs?q={}&l=United+Kingdom&sort=date&start={}&limit=50&filter=0\"\n",
    "jobs = ['data+scientist','data+analyst','data+engineer',\n",
    "        'machine+learning+engineer','decision+scientist',\n",
    "        'BI+analyst','visualisation+analyst','business+intelligence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [18:01<00:00, 135.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scrape_job_description(url,jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'scraped_data/base'\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "df_list = [pd.read_csv(file, index_col=None, header=0) for file in all_files]\n",
    "df = pd.concat(df_list, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = df['Company'].str.replace(' ','+').str.replace('\\n','').unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [17:22<00:00,  5.21s/it]\n",
      "100%|██████████| 200/200 [17:21<00:00,  5.21s/it]\n",
      "100%|██████████| 200/200 [17:19<00:00,  5.20s/it]\n",
      "100%|██████████| 200/200 [17:19<00:00,  5.20s/it]\n",
      "100%|██████████| 200/200 [17:18<00:00,  5.19s/it]\n",
      "100%|██████████| 200/200 [17:17<00:00,  5.19s/it]\n",
      "100%|██████████| 200/200 [17:22<00:00,  5.21s/it]\n",
      "100%|██████████| 200/200 [17:17<00:00,  5.19s/it]\n",
      "100%|██████████| 200/200 [17:19<00:00,  5.20s/it]\n",
      "100%|██████████| 200/200 [17:26<00:00,  5.23s/it]\n",
      "100%|██████████| 197/197 [17:14<00:00,  5.25s/it]\n"
     ]
    }
   ],
   "source": [
    "scrape_company_data(companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('rescrape.csv')\n",
    "url_list = df2['url'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [25:23<00:00,  7.62s/it]\n",
      "100%|██████████| 200/200 [25:38<00:00,  7.69s/it]\n",
      "100%|██████████| 200/200 [25:45<00:00,  7.73s/it]\n",
      "100%|██████████| 200/200 [25:09<00:00,  7.55s/it]\n",
      "100%|██████████| 200/200 [25:07<00:00,  7.54s/it]\n",
      "100%|██████████| 159/159 [19:50<00:00,  7.49s/it]\n"
     ]
    }
   ],
   "source": [
    "scrape_full_desc(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Imb",
   "language": "python",
   "name": "imb_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
